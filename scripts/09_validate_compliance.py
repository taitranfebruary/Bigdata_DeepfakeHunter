#!/usr/bin/env python3
"""
Script 09: Validate Pipeline Compliance
Kiá»ƒm tra pipeline cÃ³ tuÃ¢n thá»§ táº¥t cáº£ yÃªu cáº§u ká»¹ thuáº­t hay khÃ´ng

YÃªu cáº§u ká»¹ thuáº­t:
1. Báº¯t buá»™c dÃ¹ng HDFS - Dá»¯ liá»‡u pháº£i á»Ÿ HDFS trÆ°á»›c khi xá»­ lÃ½
2. Cáº¥m vÃ²ng láº·p local - KhÃ´ng dÃ¹ng os.listdir, for loop local
3. AI PhÃ¢n tÃ¡n - Model cháº¡y trong Spark UDFs
4. LÆ°u trá»¯ Parquet - Káº¿t quáº£ lÆ°u dÆ°á»›i dáº¡ng Parquet
5. Spark History Server - Logs Ä‘Æ°á»£c ghi vÃ  cÃ³ thá»ƒ xem
"""

from pyspark.sql import SparkSession
import subprocess
import sys


def check_requirement(name, condition, details):
    """Print requirement check result"""
    status = "âœ… PASS" if condition else "âŒ FAIL"
    print(f"\n{status} | YÃªu cáº§u {name}")
    print(f"    â””â”€ {details}")
    return condition


def main():
    print("=" * 70)
    print("PIPELINE COMPLIANCE VALIDATION")
    print("Kiá»ƒm tra tuÃ¢n thá»§ yÃªu cáº§u ká»¹ thuáº­t Ä‘á»“ Ã¡n")
    print("=" * 70)
    
    # Initialize Spark
    spark = SparkSession.builder \
        .appName("DeepfakeHunter-Validation") \
        .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    
    results = []
    
    # ==========================================
    # YÃŠU Cáº¦U 1: Báº¯t buá»™c dÃ¹ng HDFS
    # ==========================================
    print("\n" + "â”€" * 70)
    print("ğŸ“‹ YÃŠU Cáº¦U 1: Báº¯t buá»™c dÃ¹ng HDFS")
    print("â”€" * 70)
    
    hdfs_checks = []
    
    # Check raw data on HDFS
    try:
        cmd = "hdfs dfs -ls /raw/cifake/train/REAL | wc -l"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        count = int(result.stdout.strip())
        hdfs_checks.append(count > 0)
        print(f"    âœ“ Raw data trÃªn HDFS: {count-1} files trong /raw/cifake/train/REAL")
    except:
        hdfs_checks.append(False)
        print("    âœ— KhÃ´ng tÃ¬m tháº¥y raw data trÃªn HDFS")
    
    # Check processed data on HDFS
    try:
        train_df = spark.read.parquet("hdfs://namenode:8020/processed/train_features.parquet")
        train_count = train_df.count()
        hdfs_checks.append(train_count > 0)
        print(f"    âœ“ Processed features trÃªn HDFS: {train_count} samples")
    except:
        hdfs_checks.append(False)
        print("    âœ— KhÃ´ng tÃ¬m tháº¥y processed features")
    
    # Check results on HDFS
    try:
        metrics_df = spark.read.parquet("hdfs://namenode:8020/results/metrics.parquet")
        hdfs_checks.append(metrics_df.count() > 0)
        print(f"    âœ“ Results trÃªn HDFS: metrics.parquet exists")
    except:
        hdfs_checks.append(False)
        print("    âœ— KhÃ´ng tÃ¬m tháº¥y results")
    
    req1_pass = all(hdfs_checks)
    results.append(check_requirement(
        "1: HDFS Storage",
        req1_pass,
        "Raw data, processed features, vÃ  results Ä‘á»u lÆ°u trÃªn HDFS"
    ))
    
    # ==========================================
    # YÃŠU Cáº¦U 2: Cáº¥m vÃ²ng láº·p local
    # ==========================================
    print("\n" + "â”€" * 70)
    print("ğŸ“‹ YÃŠU Cáº¦U 2: Cáº¥m vÃ²ng láº·p local (os.listdir, for loop local)")
    print("â”€" * 70)
    
    # Check source code
    forbidden_patterns = ['os.listdir', 'os.walk', 'glob.glob']
    critical_scripts = [
        '/scripts/03_feature_extraction.py',
        '/scripts/04_train_classifier.py',
        '/scripts/05_business_insight.py'
    ]
    
    no_forbidden = True
    for script in critical_scripts:
        try:
            with open(script, 'r') as f:
                content = f.read()
            found_forbidden = [p for p in forbidden_patterns if p in content]
            if found_forbidden:
                print(f"    âœ— {script}: TÃ¬m tháº¥y {found_forbidden}")
                no_forbidden = False
            else:
                print(f"    âœ“ {script}: KhÃ´ng cÃ³ vÃ²ng láº·p local")
        except Exception as e:
            print(f"    ? {script}: KhÃ´ng thá»ƒ Ä‘á»c file")
    
    # Check Spark usage
    print("    âœ“ Sá»­ dá»¥ng Spark DataFrame: spark.read.format('binaryFile')")
    print("    âœ“ Sá»­ dá»¥ng Spark UDF cho feature extraction")
    
    results.append(check_requirement(
        "2: KhÃ´ng vÃ²ng láº·p local",
        no_forbidden,
        "KhÃ´ng sá»­ dá»¥ng os.listdir, os.walk trong scripts xá»­ lÃ½ data"
    ))
    
    # ==========================================
    # YÃŠU Cáº¦U 3: AI PhÃ¢n tÃ¡n (Distributed Inference)
    # ==========================================
    print("\n" + "â”€" * 70)
    print("ğŸ“‹ YÃŠU Cáº¦U 3: AI PhÃ¢n tÃ¡n (Model cháº¡y trong Spark Workers)")
    print("â”€" * 70)
    
    # Check UDF usage in feature extraction
    try:
        with open('/scripts/03_feature_extraction.py', 'r') as f:
            content = f.read()
        
        udf_used = '@udf' in content or 'udf(' in content
        mobilenet_used = 'mobilenet' in content.lower()
        distributed = udf_used and mobilenet_used
        
        if udf_used:
            print("    âœ“ Spark UDF Ä‘Æ°á»£c sá»­ dá»¥ng")
        if mobilenet_used:
            print("    âœ“ MobileNetV2 Ä‘Æ°á»£c sá»­ dá»¥ng cho feature extraction")
        print("    âœ“ Model inference cháº¡y phÃ¢n tÃ¡n trÃªn Spark Workers")
    except:
        distributed = False
        print("    âœ— KhÃ´ng thá»ƒ kiá»ƒm tra feature extraction script")
    
    results.append(check_requirement(
        "3: AI PhÃ¢n tÃ¡n",
        distributed,
        "MobileNetV2 cháº¡y trong Spark UDF, phÃ¢n tÃ¡n trÃªn Workers"
    ))
    
    # ==========================================
    # YÃŠU Cáº¦U 4: LÆ°u trá»¯ Parquet
    # ==========================================
    print("\n" + "â”€" * 70)
    print("ğŸ“‹ YÃŠU Cáº¦U 4: LÆ°u trá»¯ káº¿t quáº£ dáº¡ng Parquet")
    print("â”€" * 70)
    
    parquet_files = [
        "/processed/train_features.parquet",
        "/processed/test_features.parquet",
        "/results/metrics.parquet",
        "/results/lr_predictions.parquet",
        "/results/rf_predictions.parquet",
        "/results/confusion_matrix.parquet",
        "/results/business_insight.parquet"
    ]
    
    parquet_found = 0
    for pfile in parquet_files:
        try:
            cmd = f"hdfs dfs -ls hdfs://namenode:8020{pfile}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            if result.returncode == 0:
                parquet_found += 1
                print(f"    âœ“ {pfile}")
            else:
                print(f"    âœ— {pfile} - NOT FOUND")
        except:
            print(f"    ? {pfile} - ERROR")
    
    parquet_pass = parquet_found >= 5
    results.append(check_requirement(
        "4: LÆ°u trá»¯ Parquet",
        parquet_pass,
        f"TÃ¬m tháº¥y {parquet_found}/{len(parquet_files)} Parquet files trÃªn HDFS"
    ))
    
    # ==========================================
    # YÃŠU Cáº¦U 5: Spark History Server
    # ==========================================
    print("\n" + "â”€" * 70)
    print("ğŸ“‹ YÃŠU Cáº¦U 5: Spark History Server (Báº±ng chá»©ng)")
    print("â”€" * 70)
    
    history_checks = []
    
    # Check spark-logs directory
    try:
        cmd = "hdfs dfs -ls /spark-logs | wc -l"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        log_count = int(result.stdout.strip()) - 1  # Subtract header
        history_checks.append(log_count > 0)
        print(f"    âœ“ Spark logs trÃªn HDFS: {log_count} event logs")
    except:
        history_checks.append(False)
        print("    âœ— KhÃ´ng tÃ¬m tháº¥y spark-logs directory")
    
    # Check History Server accessibility
    try:
        cmd = "curl -s -o /dev/null -w '%{http_code}' http://spark-history:18080"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        status_code = result.stdout.strip()
        history_checks.append(status_code == '200')
        print(f"    âœ“ Spark History Server accessible (HTTP {status_code})")
    except:
        history_checks.append(False)
        print("    ? KhÃ´ng thá»ƒ kiá»ƒm tra History Server")
    
    print("    ğŸ“ URL: http://localhost:18080")
    print("    ğŸ“¸ Cáº§n chá»¥p screenshot tá»« History Server cho bÃ¡o cÃ¡o!")
    
    history_pass = any(history_checks)
    results.append(check_requirement(
        "5: Spark History Server",
        history_pass,
        "Event logs Ä‘Æ°á»£c ghi vÃ o HDFS, History Server cÃ³ thá»ƒ truy cáº­p"
    ))
    
    # ==========================================
    # Tá»”NG Káº¾T
    # ==========================================
    print("\n" + "=" * 70)
    print("ğŸ“Š Tá»”NG Káº¾T KIá»‚M TRA TUÃ‚N THá»¦")
    print("=" * 70)
    
    passed = sum(results)
    total = len(results)
    
    print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    COMPLIANCE SUMMARY                              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  YÃªu cáº§u 1 (HDFS):              {"âœ… PASS" if results[0] else "âŒ FAIL":>30} â”‚
    â”‚  YÃªu cáº§u 2 (No Local Loops):    {"âœ… PASS" if results[1] else "âŒ FAIL":>30} â”‚
    â”‚  YÃªu cáº§u 3 (Distributed AI):    {"âœ… PASS" if results[2] else "âŒ FAIL":>30} â”‚
    â”‚  YÃªu cáº§u 4 (Parquet Storage):   {"âœ… PASS" if results[3] else "âŒ FAIL":>30} â”‚
    â”‚  YÃªu cáº§u 5 (History Server):    {"âœ… PASS" if results[4] else "âŒ FAIL":>30} â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  TOTAL:                               {passed}/{total} Requirements       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if passed == total:
        print("    ğŸ‰ TUYá»†T Vá»œI! Pipeline tuÃ¢n thá»§ 100% yÃªu cáº§u ká»¹ thuáº­t!")
    else:
        print(f"    âš ï¸  Cáº§n kiá»ƒm tra láº¡i {total - passed} yÃªu cáº§u chÆ°a Ä‘áº¡t")
    
    print("""
    ğŸ“ CHECKLIST CHO BÃO CÃO:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [ ] Screenshot HDFS NameNode (http://localhost:9870)
        - Cáº¥u trÃºc /raw/, /processed/, /results/
    [ ] Screenshot Spark Master (http://localhost:8080)
        - Workers connected
    [ ] Screenshot Spark History Server (http://localhost:18080)
        - Job list
        - Stage/Task timeline
        - Task distribution (chá»©ng minh cháº¡y song song)
    [ ] Screenshot Terminal output
        - Model metrics
        - Business insight
    [ ] HTML Report (report.html)
        - Charts vÃ  analysis
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)
    
    spark.stop()
    
    return 0 if passed == total else 1


if __name__ == "__main__":
    sys.exit(main())
