#!/usr/bin/env python3
"""
Script 06: View Results - Advanced Visualization
Xem káº¿t quáº£ tá»« HDFS sau khi cháº¡y pipeline vá»›i visualization chi tiáº¿t
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, sum as spark_sum, round as spark_round


def print_banner(title, width=70):
    """Print a formatted banner"""
    print("\n" + "â•”" + "â•" * (width-2) + "â•—")
    print("â•‘" + title.center(width-2) + "â•‘")
    print("â•š" + "â•" * (width-2) + "â•")


def print_section(title, width=60):
    """Print section header"""
    print("\n" + "â”€" * width)
    print(f"ğŸ“Š {title}")
    print("â”€" * width)


def print_metric_box(metrics):
    """Print metrics in a nice box format"""
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      MODEL PERFORMANCE METRICS                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Metric           â”‚ Logistic Regressionâ”‚ Random Forest              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤""")
    
    for m in metrics:
        if m['model'] == 'LogisticRegression':
            lr = m
        else:
            rf = m
    
    print(f"    â”‚ Accuracy         â”‚ {lr['accuracy']*100:>16.2f}% â”‚ {rf['accuracy']*100:>24.2f}% â”‚")
    print(f"    â”‚ Precision        â”‚ {lr['precision']*100:>16.2f}% â”‚ {rf['precision']*100:>24.2f}% â”‚")
    print(f"    â”‚ Recall           â”‚ {lr['recall']*100:>16.2f}% â”‚ {rf['recall']*100:>24.2f}% â”‚")
    print(f"    â”‚ F1-Score         â”‚ {lr['f1_score']*100:>16.2f}% â”‚ {rf['f1_score']*100:>24.2f}% â”‚")
    print(f"    â”‚ AUC-ROC          â”‚ {lr['auc_roc']*100:>16.2f}% â”‚ {rf['auc_roc']*100:>24.2f}% â”‚")
    print(f"    â”‚ Training Time    â”‚ {lr['train_time_seconds']:>16.2f}s â”‚ {rf['train_time_seconds']:>24.2f}s â”‚")
    print("    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Determine winner
    winner = "Random Forest ğŸ†" if rf['accuracy'] > lr['accuracy'] else "Logistic Regression ğŸ†"
    print(f"\n    ğŸ¯ Best Model: {winner}")


def print_confusion_matrix(cm_data, model_name):
    """Print confusion matrix visualization"""
    for cm in cm_data:
        if cm['model'] == model_name:
            tp, tn, fp, fn = cm['true_positive'], cm['true_negative'], cm['false_positive'], cm['false_negative']
            total = tp + tn + fp + fn
            
            print(f"""
    {model_name} Confusion Matrix:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚ Predicted REAL    â”‚ Predicted FAKE    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Actual REAL         â”‚ TN = {tn:>6}       â”‚ FP = {fp:>6}       â”‚
    â”‚ Actual FAKE         â”‚ FN = {fn:>6}       â”‚ TP = {tp:>6}       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ğŸ“ˆ Derived Metrics:
       â€¢ Accuracy:    {(tp+tn)/total*100:.2f}%
       â€¢ Precision:   {tp/(tp+fp)*100:.2f}% (of predicted FAKE, how many are correct)
       â€¢ Recall:      {tp/(tp+fn)*100:.2f}% (of actual FAKE, how many detected)
       â€¢ Specificity: {tn/(tn+fp)*100:.2f}% (of actual REAL, how many correct)
       â€¢ FPR:         {fp/(tn+fp)*100:.2f}% (False Positive Rate)
       â€¢ FNR:         {fn/(tp+fn)*100:.2f}% (False Negative Rate - CRITICAL!)
            """)


def print_ascii_bar_chart(values, labels, title, max_width=40):
    """Print ASCII bar chart"""
    print(f"\n    {title}")
    print("    " + "â”€" * 50)
    max_val = max(values) if values else 1
    for label, value in zip(labels, values):
        bar_len = int((value / max_val) * max_width) if max_val > 0 else 0
        bar = "â–ˆ" * bar_len
        print(f"    {label:>15} â”‚{bar} {value:.2f}%")
    print()


def main():
    print_banner("DEEPFAKE HUNTER - RESULTS DASHBOARD", 70)
    
    # Khá»Ÿi táº¡o Spark Session
    spark = SparkSession.builder \
        .appName("DeepfakeHunter-ViewResults") \
        .config("spark.eventLog.enabled", "true") \
        .config("spark.eventLog.dir", "hdfs://namenode:8020/spark-logs") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    
    # =====================================
    # 1. View Metrics
    # =====================================
    print_section("MODEL PERFORMANCE METRICS")
    
    try:
        metrics_df = spark.read.parquet("hdfs://namenode:8020/results/metrics.parquet")
        metrics_data = metrics_df.collect()
        metrics_dict = [row.asDict() for row in metrics_data]
        print_metric_box(metrics_dict)
        
        # ASCII bar chart for comparison
        lr_acc = rf_acc = 0
        for m in metrics_dict:
            if m['model'] == 'LogisticRegression':
                lr_acc = m['accuracy'] * 100
            else:
                rf_acc = m['accuracy'] * 100
        
        print_ascii_bar_chart(
            [lr_acc, rf_acc],
            ['LogisticReg', 'RandomForest'],
            "Accuracy Comparison"
        )
        
    except Exception as e:
        print(f"âŒ Cannot read metrics: {e}")
    
    # =====================================
    # 2. View Confusion Matrix
    # =====================================
    print_section("CONFUSION MATRIX ANALYSIS")
    
    try:
        cm_df = spark.read.parquet("hdfs://namenode:8020/results/confusion_matrix.parquet")
        cm_data = [row.asDict() for row in cm_df.collect()]
        
        print_confusion_matrix(cm_data, "LogisticRegression")
        print_confusion_matrix(cm_data, "RandomForest")
        
    except Exception as e:
        print(f"âŒ Cannot read confusion matrix: {e}")
    
    # =====================================
    # 3. View Business Insight
    # =====================================
    print_section("BUSINESS INSIGHT SUMMARY")
    
    try:
        insight_df = spark.read.parquet("hdfs://namenode:8020/results/business_insight.parquet")
        
        print("\n    ğŸ“‹ Key Insights:")
        for row in insight_df.collect():
            print(f"       â€¢ {row['metric']}: {row['value']}")
            
    except Exception as e:
        print(f"âŒ Cannot read business insight: {e}")
    
    # =====================================
    # 4. Sample Predictions Analysis
    # =====================================
    print_section("PREDICTION ANALYSIS")
    
    try:
        lr_pred = spark.read.parquet("hdfs://namenode:8020/results/lr_predictions.parquet")
        rf_pred = spark.read.parquet("hdfs://namenode:8020/results/rf_predictions.parquet")
        
        lr_total = lr_pred.count()
        rf_total = rf_pred.count()
        
        print(f"\n    ğŸ“Š Prediction Statistics:")
        print(f"       Total test samples: {lr_total}")
        
        # LR breakdown
        lr_correct = lr_pred.filter(col("label") == col("prediction")).count()
        lr_wrong = lr_total - lr_correct
        
        # RF breakdown  
        rf_correct = rf_pred.filter(col("label") == col("prediction")).count()
        rf_wrong = rf_total - rf_correct
        
        print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    PREDICTION BREAKDOWN                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                     â”‚ LogisticReg      â”‚ RandomForest       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Correct Predictions â”‚ {lr_correct:>12}     â”‚ {rf_correct:>14}     â”‚
    â”‚ Wrong Predictions   â”‚ {lr_wrong:>12}     â”‚ {rf_wrong:>14}     â”‚
    â”‚ Accuracy            â”‚ {lr_correct/lr_total*100:>11.2f}%     â”‚ {rf_correct/rf_total*100:>13.2f}%     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        # Sample predictions
        print("\n    ğŸ“‹ Sample Predictions (First 10):")
        lr_pred.select("path", "label", "label_name", "prediction") \
            .withColumn("correct", when(col("label") == col("prediction"), "âœ“").otherwise("âœ—")) \
            .show(10, truncate=45)
            
    except Exception as e:
        print(f"âŒ Cannot read predictions: {e}")
    
    # =====================================
    # 5. Feature Statistics
    # =====================================
    print_section("DATASET & FEATURE STATISTICS")
    
    try:
        train_features = spark.read.parquet("hdfs://namenode:8020/processed/train_features.parquet")
        test_features = spark.read.parquet("hdfs://namenode:8020/processed/test_features.parquet")
        
        train_count = train_features.count()
        test_count = test_features.count()
        total_count = train_count + test_count
        
        print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    DATASET STATISTICS                        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Dataset: CIFAKE (Real vs AI-Generated)                      â”‚
    â”‚  Feature Extractor: MobileNetV2 (ImageNet pretrained)        â”‚
    â”‚  Feature Dimension: 1280                                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Training samples:  {train_count:>8}                                    â”‚
    â”‚  Test samples:      {test_count:>8}                                    â”‚
    â”‚  Total samples:     {total_count:>8}                                    â”‚
    â”‚  Train/Test ratio:  {train_count/total_count*100:.1f}% / {test_count/total_count*100:.1f}%                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        # Label distribution
        print("\n    ğŸ“Š Label Distribution:")
        print("\n    Training Set:")
        train_dist = train_features.groupBy("label_name").count().collect()
        for row in train_dist:
            pct = row['count'] / train_count * 100
            bar = "â–ˆ" * int(pct / 2)
            print(f"       {row['label_name']:>6}: {row['count']:>6} ({pct:.1f}%) {bar}")
        
        print("\n    Test Set:")
        test_dist = test_features.groupBy("label_name").count().collect()
        for row in test_dist:
            pct = row['count'] / test_count * 100
            bar = "â–ˆ" * int(pct / 2)
            print(f"       {row['label_name']:>6}: {row['count']:>6} ({pct:.1f}%) {bar}")
            
    except Exception as e:
        print(f"âŒ Cannot read features: {e}")
    
    # =====================================
    # 6. HDFS Storage Summary
    # =====================================
    print_section("HDFS STORAGE SUMMARY")
    
    print("""
    ğŸ“ HDFS Directory Structure:
    
    /raw/cifake/                    â† Raw image data
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ REAL/                   (~50,000 images)
    â”‚   â””â”€â”€ FAKE/                   (~50,000 images)
    â””â”€â”€ test/
        â”œâ”€â”€ REAL/                   (~10,000 images)
        â””â”€â”€ FAKE/                   (~10,000 images)
    
    /processed/                     â† Extracted features (Parquet)
    â”œâ”€â”€ train_features.parquet
    â””â”€â”€ test_features.parquet
    
    /results/                       â† Model outputs
    â”œâ”€â”€ metrics.parquet
    â”œâ”€â”€ lr_predictions.parquet
    â”œâ”€â”€ rf_predictions.parquet
    â”œâ”€â”€ confusion_matrix.parquet
    â”œâ”€â”€ business_insight.parquet
    â””â”€â”€ models/
        â”œâ”€â”€ logistic_regression/
        â””â”€â”€ random_forest/
    
    /spark-logs/                    â† Spark History Server logs
    """)
    
    # =====================================
    # 7. Final Answer to Business Question
    # =====================================
    print_banner("ANSWER TO BUSINESS QUESTION", 70)
    
    try:
        metrics_df = spark.read.parquet("hdfs://namenode:8020/results/metrics.parquet")
        best_acc = metrics_df.agg({"accuracy": "max"}).collect()[0][0]
        
        answer = "âœ… CÃ“" if best_acc > 0.7 else "âŒ CHÆ¯A Äá»¦"
        
        print(f"""
    â“ CÃ‚U Há»I: 
       "Liá»‡u model Ä‘Æ°á»£c chá»n cÃ³ trÃ­ch xuáº¥t Ä‘á»§ thÃ´ng tin Ä‘á»ƒ phÃ¡t hiá»‡n 
        Deepfake khÃ´ng?"

    ğŸ’¡ TRáº¢ Lá»œI: {answer}

    ğŸ“Š GIáº¢I THÃCH:
       â€¢ Accuracy Ä‘áº¡t Ä‘Æ°á»£c: {best_acc*100:.2f}%
       â€¢ MobileNetV2 (pretrained ImageNet) trÃ­ch xuáº¥t 1280 features
       â€¢ Features nÃ y chá»©a thÃ´ng tin vá» textures, edges, patterns
       â€¢ AI-generated images cÃ³ artifacts mÃ  model cÃ³ thá»ƒ phÃ¡t hiá»‡n:
         - Smooth textures khÃ´ng tá»± nhiÃªn
         - Inconsistent lighting/shadows
         - Subtle pattern repetitions
       â€¢ Hybrid approach (DL features + Classical ML) hoáº¡t Ä‘á»™ng tá»‘t
         vÃ  cÃ³ thá»ƒ scale trÃªn Spark cluster

    ğŸ¯ Káº¾T LUáº¬N:
       MobileNetV2 features Káº¾T Há»¢P vá»›i Spark MLlib classifiers
       {"Äá»¦ KHáº¢ NÄ‚NG" if best_acc > 0.7 else "CHÆ¯A Äá»¦ KHáº¢ NÄ‚NG"} phÃ¡t hiá»‡n Deepfake trong dataset CIFAKE
       vá»›i Ä‘á»™ chÃ­nh xÃ¡c {best_acc*100:.2f}%.
        """)
    except Exception as e:
        print(f"âŒ Cannot generate answer: {e}")
    
    spark.stop()
    
    print_banner("RESULTS VIEWING COMPLETED âœ…", 70)

if __name__ == "__main__":
    main()
