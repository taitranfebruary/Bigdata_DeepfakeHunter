services:
  spark-master:
    build:
      context: .
      dockerfile: images/spark-master/Dockerfile
    image: my-spark-master:3.3.0
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_NO_DAEMONIZE=true
      # Spark Event Log for History Server
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=hdfs://namenode:8020/spark-logs
    command: /opt/spark/sbin/start-master.sh
    volumes:
      - "./spark-spill-master:/spark-spill"
      - "./scripts:/scripts"
      - "./dataset:/scripts/dataset"
    networks: [bigdata_network]
    depends_on:
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5

  spark-worker:
    build:
      context: .
      dockerfile: images/spark-worker/Dockerfile
    image: my-spark-worker:3.3.0
    container_name: spark-worker
    depends_on: [spark-master]
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_MEMORY=10g
      - SPARK_WORKER_CORES=4
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    volumes:
      - "./spark-spill-worker:/spark-spill"
      - "./scripts:/scripts"
      - "./dataset:/scripts/dataset"
    networks: [bigdata_network]
    deploy:
      resources:
        limits:
          memory: 12g
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Spark History Server - YÊU CẦU BẮT BUỘC
  spark-history:
    build:
      context: .
      dockerfile: images/spark-master/Dockerfile
    image: my-spark-master:3.3.0
    container_name: spark-history
    depends_on:
      namenode:
        condition: service_healthy
    ports:
      - "18080:18080"
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: >
      bash -c "
        sleep 10 &&
        hdfs dfs -mkdir -p /spark-logs &&
        hdfs dfs -chmod 777 /spark-logs &&
        /opt/spark/sbin/start-history-server.sh --properties-file /opt/spark/conf/spark-defaults.conf
      "
    volumes:
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    networks: [bigdata_network]
    restart: unless-stopped

  # Optional: Second worker for better distribution
  spark-worker-2:
    build:
      context: .
      dockerfile: images/spark-worker/Dockerfile
    image: my-spark-worker:3.3.0
    container_name: spark-worker-2
    depends_on: [spark-master]
    ports:
      - "8082:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=1
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    volumes:
      - "./spark-spill-worker-2:/spark-spill"
      - "./scripts:/scripts"
      - "./dataset:/scripts/dataset"
    networks: [bigdata_network]
    profiles: ["multi-worker"]

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      # HDFS Configuration for better performance
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_blocksize=67108864
    networks: [bigdata_network]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 5

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on: [namenode]
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    networks: [bigdata_network]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9864 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

networks:
  bigdata_network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode: